#!/usr/bin/env python3
"""
Investigation script to explore potential optimizations to the original Ibis implementation.
"""

import sys
import os
import time
import pandas as pd
import numpy as np

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
import traffic_anomaly
from traffic_anomaly import sample_data

def test_parameter_variations():
    """Test if different parameters can improve performance."""
    print("="*60)
    print("TESTING PARAMETER VARIATIONS")
    print("="*60)
    
    # Create medium-sized dataset
    base_data = sample_data.changepoints_input.copy()
    
    # Scale to 20x for meaningful timing
    datasets = []
    original_entities = base_data['ID'].unique()
    max_entity_id = int(base_data['ID'].max())
    
    for i in range(20):
        data_copy = base_data.copy()
        entity_mapping = {old_id: int(old_id) + (i * (max_entity_id + 1000)) for old_id in original_entities}
        data_copy['ID'] = data_copy['ID'].map(entity_mapping)
        datasets.append(data_copy)
    
    test_data = pd.concat(datasets, ignore_index=True)
    print(f"Test dataset shape: {test_data.shape}")
    
    base_params = {
        'data': test_data,
        'value_column': 'travel_time_seconds',
        'entity_grouping_column': 'ID',
        'datetime_column': 'TimeStamp',
        'score_threshold': 0.7,
        'robust': True
    }
    
    # Test different window sizes
    window_sizes = [7, 14, 21]
    print("\nTesting different rolling window sizes:")
    for window_days in window_sizes:
        params = base_params.copy()
        params['rolling_window_days'] = window_days
        
        start_time = time.time()
        result = traffic_anomaly.changepoint(**params)
        elapsed = time.time() - start_time
        
        print(f"  {window_days} days window: {elapsed:.2f}s -> {result.shape}")
    
    # Test different quantile bounds
    print("\nTesting different quantile bounds:")
    bounds_configs = [
        (0.05, 0.95),  # Default
        (0.1, 0.9),    # Less extreme
        (0.01, 0.99)   # More extreme
    ]
    
    for lower, upper in bounds_configs:
        params = base_params.copy()
        params['lower_bound'] = lower
        params['upper_bound'] = upper
        params['rolling_window_days'] = 14
        
        start_time = time.time()
        result = traffic_anomaly.changepoint(**params)
        elapsed = time.time() - start_time
        
        print(f"  Bounds ({lower}, {upper}): {elapsed:.2f}s -> {result.shape}")

def analyze_ibis_sql_query():
    """Analyze the SQL query generated by Ibis to understand bottlenecks."""
    print("\n" + "="*60)
    print("ANALYZING IBIS SQL QUERY")
    print("="*60)
    
    # Use small dataset to get readable SQL
    small_data = sample_data.changepoints_input.head(100)
    
    # Get SQL for robust=True
    sql_query = traffic_anomaly.changepoint(
        data=small_data,
        value_column='travel_time_seconds',
        entity_grouping_column='ID',
        datetime_column='TimeStamp',
        score_threshold=0.7,
        robust=True,
        return_sql=True
    )
    
    print("Generated SQL Query (truncated):")
    print("=" * 40)
    # Print first 2000 characters to see the structure
    print(sql_query[:2000])
    if len(sql_query) > 2000:
        print("... [truncated] ...")
        print(f"Total SQL length: {len(sql_query)} characters")
    
    # Count joins and subqueries
    join_count = sql_query.lower().count('join')
    subquery_count = sql_query.count('(SELECT') + sql_query.count('( SELECT')
    
    print(f"\nSQL Complexity Analysis:")
    print(f"  Number of JOINs: {join_count}")
    print(f"  Number of subqueries: {subquery_count}")
    print(f"  Total query length: {len(sql_query)} characters")

def profile_ibis_execution():
    """Profile the Ibis execution to understand where time is spent."""
    print("\n" + "="*60)
    print("PROFILING IBIS EXECUTION")
    print("="*60)
    
    # Create medium dataset for profiling
    base_data = sample_data.changepoints_input.copy()
    datasets = []
    original_entities = base_data['ID'].unique()
    max_entity_id = int(base_data['ID'].max())
    
    for i in range(10):
        data_copy = base_data.copy()
        entity_mapping = {old_id: int(old_id) + (i * (max_entity_id + 1000)) for old_id in original_entities}
        data_copy['ID'] = data_copy['ID'].map(entity_mapping)
        datasets.append(data_copy)
    
    test_data = pd.concat(datasets, ignore_index=True)
    
    print(f"Profiling dataset shape: {test_data.shape}")
    
    # Time different phases
    print("\nTiming different execution phases:")
    
    # Phase 1: Data conversion
    start_time = time.time()
    import ibis
    table = ibis.memtable(test_data)
    conversion_time = time.time() - start_time
    print(f"  Data conversion to Ibis: {conversion_time:.3f}s")
    
    # Phase 2: Query building (robust=False)
    start_time = time.time()
    query_false = traffic_anomaly.changepoint(
        data=table,
        value_column='travel_time_seconds',
        entity_grouping_column='ID',
        datetime_column='TimeStamp',
        score_threshold=0.7,
        robust=False,
        return_sql=True
    )
    query_build_false_time = time.time() - start_time
    print(f"  Query building (robust=False): {query_build_false_time:.3f}s")
    
    # Phase 3: Query building (robust=True)
    start_time = time.time()
    query_true = traffic_anomaly.changepoint(
        data=table,
        value_column='travel_time_seconds',
        entity_grouping_column='ID',
        datetime_column='TimeStamp',
        score_threshold=0.7,
        robust=True,
        return_sql=True
    )
    query_build_true_time = time.time() - start_time
    print(f"  Query building (robust=True): {query_build_true_time:.3f}s")
    
    # Phase 4: Query execution
    start_time = time.time()
    result_false = traffic_anomaly.changepoint(
        data=table,
        value_column='travel_time_seconds',
        entity_grouping_column='ID',
        datetime_column='TimeStamp',
        score_threshold=0.7,
        robust=False
    )
    exec_false_time = time.time() - start_time
    print(f"  Query execution (robust=False): {exec_false_time:.3f}s")
    
    start_time = time.time()
    result_true = traffic_anomaly.changepoint(
        data=table,
        value_column='travel_time_seconds',
        entity_grouping_column='ID',
        datetime_column='TimeStamp',
        score_threshold=0.7,
        robust=True
    )
    exec_true_time = time.time() - start_time
    print(f"  Query execution (robust=True): {exec_true_time:.3f}s")
    
    print(f"\nExecution time breakdown:")
    print(f"  robust=False total: {exec_false_time:.3f}s")
    print(f"  robust=True total: {exec_true_time:.3f}s")
    print(f"  Slowdown factor: {exec_true_time/exec_false_time:.1f}x")

if __name__ == "__main__":
    print("Investigating Ibis implementation optimization opportunities...")
    
    test_parameter_variations()
    analyze_ibis_sql_query()
    profile_ibis_execution()
    
    print("\n" + "="*60)
    print("INVESTIGATION COMPLETE")
    print("="*60)